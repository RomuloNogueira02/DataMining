{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3987eef7-af22-4633-a6ac-1ac3b1a6be76",
   "metadata": {},
   "source": [
    "# Prospecção de Dados (Data Mining) DI/FCUL - HA3\n",
    "\n",
    "## Third Home Assignement (MC/DI/FCUL - 2024)\n",
    "\n",
    "### Fill in the section below\n",
    "\n",
    "### GROUP:`01` \n",
    "\n",
    "* Catherine Prokhorov (62608) - `X` Hours worked on the project\n",
    "* Guilherme Cepeda (62931) - `X` Hours worked on the project\n",
    "* Jorge Aleluia (54549) - `X` Hours worked on the project \n",
    "* Rómulo Nogueira (56935) - `X` Hours worked on the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae0373-c651-4cfe-b4a4-42e9db6bd7e6",
   "metadata": {},
   "source": [
    "The purpose of this Home Assignment is:\n",
    "\n",
    "- Find similar items with Local Sensitivity Hashing\n",
    "- Do Dimensionality Reduction\n",
    "\n",
    "**NOTE 1: Students are not allowed to add more cells to the notebook**\n",
    "\n",
    "**NOTE 2: The notebook must be submited fully executed**\n",
    "\n",
    "**NOTE 3: Name of notebook should be: HA3_GROUP-XX.ipynb (where XX is the group number)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a40f24-0da5-4b96-90f3-84259cbd1bc5",
   "metadata": {},
   "source": [
    "### 1. Read the Dataset\n",
    "\n",
    "The dataset correspond to about 99% of the Human Proteome (set of known Human Proteins - about 19,500), coded with specific structural elements. They are presented in a dictionary where the key is the [UniprotID](https://www.uniprot.org/) of the protein and the value is a set of indices of a specific structural characteristic\n",
    "\n",
    "\n",
    "Students can use one of two datasets, that are not subsets of each other:\n",
    "- `data_d3.pickle` - smaller set of structural features (2048)\n",
    "- `data_d4.pickle` - much larger set of structural features (20736) **Note**: This dataset has been Zipped to fit into moodle. Students should unzip it before usage\n",
    "\n",
    "Select **one** of the datasets and perform all analyses with it.\n",
    "\n",
    "It may be adviseable the usage of sparse matrices, especially for the `d4` dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7c890",
   "metadata": {},
   "source": [
    "proteins_ids = docs\n",
    "\n",
    "set_characteristics = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce8656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 19258 human proteins\n",
      "And a total of 20735 specific structural characteristics\n",
      "This would result in a matrix of size 20735x19258\n",
      "Dok matrix size (in MB):  413.93354511260986\n",
      "CSR matrix size (in MB):  115.15146541595459\n"
     ]
    }
   ],
   "source": [
    "### Your code here\n",
    "import pickle\n",
    "import sys\n",
    "from scipy.sparse import dok_array, csr_array, csc_array, bsr_array, lil_array\n",
    "import numpy as np\n",
    "\n",
    "f = open(\"data_d4.pickle\", \"rb\")\n",
    "\n",
    "# Load the data\n",
    "data = pickle.load(f)\n",
    "proteins_ids = set(data.keys())\n",
    "set_characteristics = list(data.values())\n",
    "max_characteristics = max([max(x) for x in set_characteristics])\n",
    "len_characteristics = max_characteristics + 1\n",
    "\n",
    "# Some informations\n",
    "print(\"We have a total of {} human proteins\".format(len(proteins_ids)))\n",
    "print(\"And a total of {} specific structural characteristics\".format(len_characteristics))\n",
    "print(\"This would result in a matrix of size {}x{}\".format(len_characteristics, len(proteins_ids)))\n",
    "\n",
    "# Create the matrix\n",
    "print()\n",
    "\n",
    "def make_sparse_matrix(set_characteristics: list[set], proteins_ids:set, len_characteristics:int):\n",
    "    N = len(proteins_ids) \n",
    "    M = len_characteristics\n",
    "\n",
    "    S_Mat = dok_array((N, M), dtype=np.int8)    \n",
    "    for i in range(N):\n",
    "        characteristics_idxs = np.array([x for x in set_characteristics[i]])\n",
    "        S_Mat[i, characteristics_idxs] = 1\n",
    "\n",
    "    return S_Mat\n",
    "\n",
    "print()\n",
    "\n",
    "dok_matrix = make_sparse_matrix(set_characteristics, proteins_ids, len_characteristics)\n",
    "dok = pickle.dumps(dok_matrix)\n",
    "\n",
    "print(\"Dok matrix size (in MB): \", sys.getsizeof(dok)/(1024*1024))\n",
    "\n",
    "# Yet Dok sparse matrices, if they are pratical for data loading, they can be very inconvenient for any type of matrix processing so they normally need to be converted to another sparse matrix format, such as CSR or CSC.\n",
    "print()\n",
    "# Convert to CSR\n",
    "csr_matrix = csr_array(dok_matrix)\n",
    "csr = pickle.dumps(csr_matrix)\n",
    "print(\"CSR matrix size (in MB): \", sys.getsizeof(csr)/(1024*1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73946fe0-a7ee-4554-87f5-6c8efc3a275f",
   "metadata": {},
   "source": [
    "### 2. Perform Local Sensitivity Hashing (LSH)\n",
    "\n",
    "- examine the selected dataset in terms of similarities and select a set of LSH parameters able to capture the most similar proteins\n",
    "- Comment your results\n",
    "\n",
    "**BONUS POINTS**: It might be interesting to identify **some** of the candidate pairs in Uniprot, to check if they share some of the same properties (e.g. for [protein P28223](https://www.uniprot.org/uniprotkb/P28223/entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de744174-1989-42af-9e78-0948ea801ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add supporting functions here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e664c47-38e6-41dc-b5b3-c2c43f033cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add processing code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a016c52-c66e-4499-8233-5631923eb695",
   "metadata": {},
   "source": [
    "##### Your short analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853614f3-1e1b-4481-9784-e29dd0b453cb",
   "metadata": {},
   "source": [
    "### 3. Do dimensionality reduction\n",
    "\n",
    "Use the techniques discussued in class to make an appropriate dimensional reduction of the selected dataset. It is not necesary to be extensive, **it is better to select one approach and do it well than try a lot of techniques with poor insights and analysis**\n",
    "\n",
    "It is important to do some sensitivity analysis, relating the dataset size reduction to the loss of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead806d9-8073-4fc0-9668-68ad0b89077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add supporting functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad80328c-ffba-4bb6-8ce3-03726ee82348",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add processing code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5925562c-ef3d-4c2d-b8bc-c9a88c72f53c",
   "metadata": {},
   "source": [
    "##### Your short analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf866497",
   "metadata": {},
   "source": [
    "### 3. Discuss your findings [to fill on your own]\n",
    "\n",
    "- Comment your results above\n",
    "- Discuss how could they be used for the full Uniprot that currently has about [248 Million proteins](https://www.uniprot.org/uniprotkb/statistics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
